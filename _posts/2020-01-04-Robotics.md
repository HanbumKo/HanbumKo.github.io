---
title: Robotics
updated: 2021-01-04 22:29

---



## Paper list

* DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills [^1]



# DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills

reference 움직임들을 이용해 강화학습 덧붙여 자연스러운 동작 + reference 동작과 goal(task)가 주어질 때 주어진 goal을 해결하는 것이 목표

1. training with a multi-clip reward based on a max operator
2. training a policy to perform multiple diverse skills that can be triggered by the user
3. sequencing multiple sigle-clip policies by using their value functions to evaluate the feasibility of transitions.



## Approaches

* Kinetic Models

  - 많은 데이터 있을때 효과적
  - latent representation 위해 gaussian process, autoencoer 사용
  - 문제가 복잡하면 잘 동작하지 않음.

* Physics-based Models

  - 사람의 직관에 의존
  - GAIL (paper link TBA)
  - (TBA)

* RL

  - 학습된 모습이 사람이 보기에 자연스럽지 못함

  - (TBA)

* Motion Imitation

  - (TBA)



reference 움직임은 목표 동작의 연속 
$$
\left\{ { \hat { q }  }_{ t } \right\}
$$
로 표현

control policy는 
$$
\pi(a_t|s_t,g_t)
$$
로 표현

$$a_t$$는 관절들의 토크들 계산에 이용됨 -> PD(proportional-derivative) 컨트롤러의 target angle

뉴럴넷 사용하였고 PPO알고리즘으로 학습
$$
\left\{ { \hat { q }  }_{ t } \right\}
$$
(reference motion)은 kinetic information을 제공함



* State $$S$$
  - 각 연결부위의 위치, 각도(quaternions)
  - Linear & angular 속도
  - Phase variable $$\phi$$

Phase variable


$$
\phi \in \left[ 0,1 \right]
$$
: $$\phi=0$$는 움직임의 시작, $$\phi=1$$은 끝을 나타낸다. 매 사이클마다 0로 초기화 한다.



#### Model

Action network: fully-connected (input -> 1024 -> 512 -> output)

Value network: fully-connected (1024 -> 512 -> 1)

vision-based task같은 경우에 주어지는 heightmap $$H$$ 는 convolutional 통과시킴 (H -> 8x8 -> 4x4 -> 4x4 -> 64 -> concatenation)



#### Reward

$$
r_{t}=w^{I}r_t^{I}+w^{G} r_t^{G}
$$



$$w^I$$: imitation weight

$$r_t^{I}$$: imitation reward

$$w^G$$: goal weight

$$r_t^{G}$$: goal reward


$$
r_t^I = w^pr_t^p + w^vr_t^v + w^er_t^e + w^cr_t^c
$$


$$w^p$$: 0.65

$$r_t^p$$: pose reward -> 관절 방향 차이(quaternion)

$$w^v$$: 0.1

$$r_t^v$$: velocity reward -> 관절 속도 차이(quaternion)

$$w^e$$: 0.15

$$r_t^e$$: end-effector reward -> 왼발 오른발 왼손 오른손 위치차이(meter)

$$w^c$$: 0.1

$$r_t^c$$: penalize center of mass


$$
{ r }_{ t }^{ p }=exp\left[ -2\left( \sum _{ j }^{  }{ \left\| { \hat { q }  }_{ t }^{ j }\circleddash { q }_{ t }^{ j } \right\|^2  }  \right)  \right]
$$

$$
{ r }_{ t }^{ v }=exp\left[ -0.1\left( \sum _{ j }^{  }{ \left\| { \hat { \dot { q }  }  }_{ t }^{ j }- { \dot { q }  }_{ t }^{ j } \right\| ^{ 2 } }  \right)  \right]
$$

$$
{ r }_{ t }^{ e }=exp\left[ -40\left( \sum _{ e }^{  }{ \left\| { \hat { p }  }_{ t }^{ e }-{ p }_{ t }^{ e } \right\| ^{ 2 } }  \right)  \right]
$$

$$
e\in[왼발, 오른발, 왼손, 오른손]
$$


$$
{ r }_{ t }^{ c }=exp\left[ -10\left( \left\| { \hat { p }  }_{ t }^{ c }-{ p }_{ t }^{ c } \right\| ^{ 2 } \right)  \right]
$$


$$\circleddash$$: quaternion differences
$$
\left\|  \right\|
$$
: scalar rotation





(TBA)











[^1]: [https://arxiv.org/abs/1804.02717](https://arxiv.org/abs/1804.02717)

